{
  "version": "2.0.0",
  "lastUpdated": "2025-01-30",
  "categories": [
    {
      "id": "basics",
      "name": "åŸºç¡€çŸ¥è¯†",
      "icon": "ğŸ“š",
      "chapters": [
        {
          "id": "dl-fundamentals",
          "title": "æ·±åº¦å­¦ä¹ åŸºç¡€",
          "icon": "ğŸ§ ",
          "sections": {
            "terminology": {
              "title": "ä¸“ä¸šæœ¯è¯­",
              "items": [
                {"term": "ç¥ç»ç½‘ç»œ", "english": "Neural Network", "desc": "ç”±å¤šå±‚äººå·¥ç¥ç»å…ƒç»„æˆçš„è®¡ç®—æ¨¡å‹ï¼Œæ¨¡æ‹Ÿç”Ÿç‰©ç¥ç»ç³»ç»Ÿ"},
                {"term": "åå‘ä¼ æ’­", "english": "Backpropagation", "desc": "è®¡ç®—æ¢¯åº¦å¹¶ä»è¾“å‡ºå±‚å‘è¾“å…¥å±‚ä¼ æ’­è¯¯å·®çš„è®­ç»ƒç®—æ³•"},
                {"term": "æ¿€æ´»å‡½æ•°", "english": "Activation Function", "desc": "å¼•å…¥éçº¿æ€§çš„å‡½æ•°ï¼Œå¦‚ReLUã€Sigmoidã€Tanh"},
                {"term": "æŸå¤±å‡½æ•°", "english": "Loss Function", "desc": "è¡¡é‡é¢„æµ‹å€¼ä¸çœŸå®å€¼å·®å¼‚çš„å‡½æ•°"},
                {"term": "æ¢¯åº¦ä¸‹é™", "english": "Gradient Descent", "desc": "æ²¿æ¢¯åº¦åæ–¹å‘æ›´æ–°å‚æ•°çš„ä¼˜åŒ–ç®—æ³•"},
                {"term": "å­¦ä¹ ç‡", "english": "Learning Rate", "desc": "æ§åˆ¶æ¯æ¬¡å‚æ•°æ›´æ–°æ­¥é•¿çš„è¶…å‚æ•°"},
                {"term": "è¿‡æ‹Ÿåˆ", "english": "Overfitting", "desc": "æ¨¡å‹åœ¨è®­ç»ƒé›†ä¸Šè¡¨ç°å¥½ä½†æ³›åŒ–èƒ½åŠ›å·®"},
                {"term": "æ­£åˆ™åŒ–", "english": "Regularization", "desc": "é˜²æ­¢è¿‡æ‹Ÿåˆçš„æŠ€æœ¯ï¼Œå¦‚L1/L2æ­£åˆ™åŒ–ã€Dropout"}
              ]
            },
            "basic": {
              "title": "åŸºç¡€æ¦‚å¿µ",
              "content": "### ä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ï¼Ÿ\n\næ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œä½¿ç”¨å¤šå±‚ç¥ç»ç½‘ç»œè‡ªåŠ¨å­¦ä¹ æ•°æ®çš„å±‚æ¬¡åŒ–è¡¨ç¤ºã€‚\n\n### æ ¸å¿ƒç»„ä»¶\n\n1. **ç¥ç»å…ƒï¼ˆNeuronï¼‰**\n   - æ¥æ”¶è¾“å…¥ï¼ŒåŠ æƒæ±‚å’Œï¼Œé€šè¿‡æ¿€æ´»å‡½æ•°è¾“å‡º\n   - å…¬å¼ï¼š`y = f(Î£(wi * xi) + b)`\n\n2. **å±‚ï¼ˆLayerï¼‰**\n   - è¾“å…¥å±‚ï¼šæ¥æ”¶åŸå§‹æ•°æ®\n   - éšè—å±‚ï¼šç‰¹å¾æå–å’Œè½¬æ¢\n   - è¾“å‡ºå±‚ï¼šäº§ç”Ÿæœ€ç»ˆç»“æœ\n\n3. **å‰å‘ä¼ æ’­**\n   - æ•°æ®ä»è¾“å…¥å±‚æµå‘è¾“å‡ºå±‚\n   - æ¯å±‚è¿›è¡Œçº¿æ€§å˜æ¢+éçº¿æ€§æ¿€æ´»\n\n4. **åå‘ä¼ æ’­**\n   - è®¡ç®—æŸå¤±å‡½æ•°å¯¹å‚æ•°çš„æ¢¯åº¦\n   - ä½¿ç”¨é“¾å¼æ³•åˆ™é€å±‚è®¡ç®—"
            },
            "advanced": {
              "title": "è¿›é˜¶çŸ¥è¯†",
              "content": "### ä¼˜åŒ–å™¨è¯¦è§£\n\n| ä¼˜åŒ–å™¨ | ç‰¹ç‚¹ | é€‚ç”¨åœºæ™¯ |\n|--------|------|----------|\n| SGD | åŸºç¡€ï¼Œå¯èƒ½éœ‡è¡ | ç®€å•ä»»åŠ¡ |\n| Momentum | åŠ é€Ÿæ”¶æ•› | é€šç”¨ |\n| Adam | è‡ªé€‚åº”å­¦ä¹ ç‡ | å¤§å¤šæ•°åœºæ™¯ |\n| AdamW | Adam+æƒé‡è¡°å‡ | Transformerè®­ç»ƒ |\n\n### æ­£åˆ™åŒ–æŠ€æœ¯\n\n1. **L1æ­£åˆ™åŒ–**ï¼šäº§ç”Ÿç¨€ç–æƒé‡\n2. **L2æ­£åˆ™åŒ–**ï¼šé˜²æ­¢æƒé‡è¿‡å¤§\n3. **Dropout**ï¼šéšæœºä¸¢å¼ƒç¥ç»å…ƒ\n4. **Batch Normalization**ï¼šæ ‡å‡†åŒ–å±‚è¾“å…¥\n5. **Layer Normalization**ï¼šTransformerå¸¸ç”¨\n\n### å­¦ä¹ ç‡è°ƒåº¦\n\n```python\n# ä½™å¼¦é€€ç«\nscheduler = CosineAnnealingLR(optimizer, T_max=100)\n\n# å¸¦çƒ­é‡å¯\nscheduler = CosineAnnealingWarmRestarts(optimizer, T_0=10)\n```"
            },
            "practice": {
              "title": "å®æˆ˜ä»£ç ",
              "content": "### PyTorch å®ç°ç®€å•ç¥ç»ç½‘ç»œ\n\n```python\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\n# å®šä¹‰ç½‘ç»œ\nclass SimpleNN(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super().__init__()\n        self.layers = nn.Sequential(\n            nn.Linear(input_size, hidden_size),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(hidden_size, hidden_size),\n            nn.ReLU(),\n            nn.Linear(hidden_size, output_size)\n        )\n    \n    def forward(self, x):\n        return self.layers(x)\n\n# åˆå§‹åŒ–\nmodel = SimpleNN(784, 256, 10)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=1e-3)\n\n# è®­ç»ƒå¾ªç¯\nfor epoch in range(num_epochs):\n    for batch_x, batch_y in dataloader:\n        optimizer.zero_grad()\n        outputs = model(batch_x)\n        loss = criterion(outputs, batch_y)\n        loss.backward()\n        optimizer.step()\n```\n\n### TensorFlow/Keras å®ç°\n\n```python\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models\n\nmodel = models.Sequential([\n    layers.Dense(256, activation='relu', input_shape=(784,)),\n    layers.Dropout(0.2),\n    layers.Dense(256, activation='relu'),\n    layers.Dense(10, activation='softmax')\n])\n\nmodel.compile(\n    optimizer='adam',\n    loss='sparse_categorical_crossentropy',\n    metrics=['accuracy']\n)\n\nmodel.fit(x_train, y_train, epochs=10, batch_size=32)\n```"
            }
          }
        },
        {
          "id": "transformer",
          "title": "Transformeræ¶æ„",
          "icon": "ğŸ”",
          "sections": {
            "terminology": {
              "title": "ä¸“ä¸šæœ¯è¯­",
              "items": [
                {"term": "æ³¨æ„åŠ›æœºåˆ¶", "english": "Attention Mechanism", "desc": "åŠ¨æ€å…³æ³¨è¾“å…¥åºåˆ—ä¸åŒéƒ¨åˆ†çš„æœºåˆ¶"},
                {"term": "è‡ªæ³¨æ„åŠ›", "english": "Self-Attention", "desc": "Qã€Kã€Væ¥è‡ªåŒä¸€è¾“å…¥çš„æ³¨æ„åŠ›"},
                {"term": "å¤šå¤´æ³¨æ„åŠ›", "english": "Multi-Head Attention", "desc": "å¹¶è¡Œå¤šä¸ªæ³¨æ„åŠ›å¤´æ•è·ä¸åŒä¿¡æ¯"},
                {"term": "ä½ç½®ç¼–ç ", "english": "Positional Encoding", "desc": "ä¸ºåºåˆ—æ·»åŠ ä½ç½®ä¿¡æ¯"},
                {"term": "å‰é¦ˆç½‘ç»œ", "english": "Feed-Forward Network", "desc": "Transformerä¸­çš„MLPå±‚"},
                {"term": "æ®‹å·®è¿æ¥", "english": "Residual Connection", "desc": "è·³è·ƒè¿æ¥ï¼Œç¼“è§£æ¢¯åº¦æ¶ˆå¤±"},
                {"term": "å±‚å½’ä¸€åŒ–", "english": "Layer Normalization", "desc": "å¯¹æ¯ä¸ªæ ·æœ¬çš„ç‰¹å¾å½’ä¸€åŒ–"}
              ]
            },
            "basic": {
              "title": "åŸºç¡€æ¦‚å¿µ",
              "content": "### Transformer æ ¸å¿ƒæ€æƒ³\n\n2017å¹´ \"Attention is All You Need\" è®ºæ–‡æå‡ºï¼Œå®Œå…¨åŸºäºæ³¨æ„åŠ›æœºåˆ¶ï¼ŒæŠ›å¼ƒRNN/CNNã€‚\n\n### è‡ªæ³¨æ„åŠ›è®¡ç®—\n\n```\nAttention(Q, K, V) = softmax(QK^T / âˆšd_k) V\n```\n\n- **Q (Query)**ï¼šæŸ¥è¯¢å‘é‡\n- **K (Key)**ï¼šé”®å‘é‡\n- **V (Value)**ï¼šå€¼å‘é‡\n- **âˆšd_k**ï¼šç¼©æ”¾å› å­ï¼Œé˜²æ­¢ç‚¹ç§¯è¿‡å¤§\n\n### æ¶æ„ç»„æˆ\n\n```\nè¾“å…¥ â†’ åµŒå…¥ + ä½ç½®ç¼–ç \n     â†“\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚  å¤šå¤´è‡ªæ³¨æ„åŠ›   â”‚ â†â”€â”\nâ”‚  Add & Norm     â”‚   â”‚ æ®‹å·®\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”€â”€â”˜\n     â†“\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚  å‰é¦ˆç½‘ç»œ       â”‚ â†â”€â”\nâ”‚  Add & Norm     â”‚   â”‚ æ®‹å·®\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”€â”€â”˜\n     â†“\n   è¾“å‡º\n```"
            },
            "advanced": {
              "title": "è¿›é˜¶çŸ¥è¯†",
              "content": "### å¤šå¤´æ³¨æ„åŠ›è¯¦è§£\n\n```python\n# å°† d_model åˆ†æˆ h ä¸ªå¤´\nhead_dim = d_model // num_heads\n\n# æ¯ä¸ªå¤´ç‹¬ç«‹è®¡ç®—æ³¨æ„åŠ›\nfor i in range(num_heads):\n    Q_i = Q @ W_Q[i]  # (seq_len, head_dim)\n    K_i = K @ W_K[i]\n    V_i = V @ W_V[i]\n    head_i = Attention(Q_i, K_i, V_i)\n\n# æ‹¼æ¥æ‰€æœ‰å¤´\noutput = Concat(head_1, ..., head_h) @ W_O\n```\n\n### ä½ç½®ç¼–ç \n\n**æ­£å¼¦ä½ç½®ç¼–ç ï¼š**\n```\nPE(pos, 2i) = sin(pos / 10000^(2i/d))\nPE(pos, 2i+1) = cos(pos / 10000^(2i/d))\n```\n\n**å¯å­¦ä¹ ä½ç½®ç¼–ç ï¼š**\n- ç›´æ¥å­¦ä¹ ä½ç½®åµŒå…¥\n- RoPEï¼ˆæ—‹è½¬ä½ç½®ç¼–ç ï¼‰\n- ALiBiï¼ˆçº¿æ€§åç½®ï¼‰\n\n### KV Cache ä¼˜åŒ–\n\næ¨ç†æ—¶ç¼“å­˜å†å² Kã€Vï¼Œé¿å…é‡å¤è®¡ç®—ï¼š\n```python\n# ç¬¬ä¸€æ¬¡ï¼šè®¡ç®—å®Œæ•´åºåˆ—\nK_cache = K[:seq_len]\nV_cache = V[:seq_len]\n\n# åç»­ï¼šåªè®¡ç®—æ–°token\nK_new = new_token @ W_K\nK_cache = concat(K_cache, K_new)\n```"
            },
            "practice": {
              "title": "å®æˆ˜ä»£ç ",
              "content": "### PyTorch å®ç° Transformer\n\n```python\nimport torch\nimport torch.nn as nn\nimport math\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, d_model, num_heads):\n        super().__init__()\n        self.d_model = d_model\n        self.num_heads = num_heads\n        self.head_dim = d_model // num_heads\n        \n        self.W_q = nn.Linear(d_model, d_model)\n        self.W_k = nn.Linear(d_model, d_model)\n        self.W_v = nn.Linear(d_model, d_model)\n        self.W_o = nn.Linear(d_model, d_model)\n    \n    def forward(self, q, k, v, mask=None):\n        batch_size = q.size(0)\n        \n        # çº¿æ€§å˜æ¢å¹¶åˆ†å¤´\n        Q = self.W_q(q).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n        K = self.W_k(k).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n        V = self.W_v(v).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n        \n        # æ³¨æ„åŠ›è®¡ç®—\n        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)\n        if mask is not None:\n            scores = scores.masked_fill(mask == 0, -1e9)\n        attn = torch.softmax(scores, dim=-1)\n        \n        # è¾“å‡º\n        out = torch.matmul(attn, V)\n        out = out.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n        return self.W_o(out)\n\nclass TransformerBlock(nn.Module):\n    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n        super().__init__()\n        self.attention = MultiHeadAttention(d_model, num_heads)\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.ffn = nn.Sequential(\n            nn.Linear(d_model, d_ff),\n            nn.GELU(),\n            nn.Linear(d_ff, d_model)\n        )\n        self.dropout = nn.Dropout(dropout)\n    \n    def forward(self, x, mask=None):\n        # è‡ªæ³¨æ„åŠ› + æ®‹å·®\n        attn_out = self.attention(x, x, x, mask)\n        x = self.norm1(x + self.dropout(attn_out))\n        \n        # FFN + æ®‹å·®\n        ffn_out = self.ffn(x)\n        x = self.norm2(x + self.dropout(ffn_out))\n        return x\n```\n\n### ä½¿ç”¨ Hugging Face Transformers\n\n```python\nfrom transformers import AutoModel, AutoTokenizer\n\n# åŠ è½½é¢„è®­ç»ƒæ¨¡å‹\nmodel = AutoModel.from_pretrained('bert-base-chinese')\ntokenizer = AutoTokenizer.from_pretrained('bert-base-chinese')\n\n# ç¼–ç æ–‡æœ¬\ninputs = tokenizer('ä½ å¥½ä¸–ç•Œ', return_tensors='pt')\noutputs = model(**inputs)\n\n# è·å–è¡¨ç¤º\nlast_hidden = outputs.last_hidden_state  # (1, seq_len, 768)\npooled = outputs.pooler_output  # (1, 768)\n```"
            }
          }
        },
        {
          "id": "llm",
          "title": "å¤§è¯­è¨€æ¨¡å‹",
          "icon": "ğŸ’¬",
          "sections": {
            "terminology": {
              "title": "ä¸“ä¸šæœ¯è¯­",
              "items": [
                {"term": "é¢„è®­ç»ƒ", "english": "Pre-training", "desc": "åœ¨å¤§è§„æ¨¡æ•°æ®ä¸Šå­¦ä¹ é€šç”¨è¡¨ç¤º"},
                {"term": "å¾®è°ƒ", "english": "Fine-tuning", "desc": "åœ¨ç‰¹å®šä»»åŠ¡æ•°æ®ä¸Šç»§ç»­è®­ç»ƒ"},
                {"term": "æŒ‡ä»¤å¾®è°ƒ", "english": "Instruction Tuning", "desc": "ç”¨æŒ‡ä»¤-å›å¤å¯¹è®­ç»ƒæ¨¡å‹éµå¾ªæŒ‡ä»¤"},
                {"term": "ä¸Šä¸‹æ–‡å­¦ä¹ ", "english": "In-Context Learning", "desc": "é€šè¿‡ç¤ºä¾‹åœ¨æ¨ç†æ—¶å­¦ä¹ "},
                {"term": "æ€ç»´é“¾", "english": "Chain-of-Thought", "desc": "è®©æ¨¡å‹å±•ç¤ºæ¨ç†è¿‡ç¨‹"},
                {"term": "æ¶Œç°èƒ½åŠ›", "english": "Emergent Abilities", "desc": "è§„æ¨¡è¾¾åˆ°é˜ˆå€¼åå‡ºç°çš„èƒ½åŠ›"},
                {"term": "å¹»è§‰", "english": "Hallucination", "desc": "æ¨¡å‹ç”Ÿæˆè™šå‡æˆ–é”™è¯¯ä¿¡æ¯"},
                {"term": "å¯¹é½", "english": "Alignment", "desc": "ä½¿æ¨¡å‹è¡Œä¸ºç¬¦åˆäººç±»æ„å›¾"}
              ]
            },
            "basic": {
              "title": "åŸºç¡€æ¦‚å¿µ",
              "content": "### ä¸»æµå¤§æ¨¡å‹\n\n| æ¨¡å‹ | å…¬å¸ | å‚æ•°é‡ | ç‰¹ç‚¹ |\n|------|------|--------|------|\n| GPT-4 | OpenAI | ~1.8T | æœ€å¼ºé€šç”¨èƒ½åŠ› |\n| Claude | Anthropic | - | å®‰å…¨ã€é•¿ä¸Šä¸‹æ–‡ |\n| Llama 3 | Meta | 8B-405B | å¼€æºæœ€å¼º |\n| Qwen 2.5 | é˜¿é‡Œ | 0.5B-72B | ä¸­æ–‡ä¼˜ç§€ |\n| DeepSeek | æ·±åº¦æ±‚ç´¢ | 7B-236B | æ€§ä»·æ¯”é«˜ |\n\n### è®­ç»ƒä¸‰é˜¶æ®µ\n\n```\n1. é¢„è®­ç»ƒ (Pre-training)\n   â””â”€ æµ·é‡æ–‡æœ¬ï¼Œè‡ªå›å½’é¢„æµ‹ä¸‹ä¸€ä¸ªtoken\n\n2. ç›‘ç£å¾®è°ƒ (SFT)\n   â””â”€ é«˜è´¨é‡æŒ‡ä»¤æ•°æ®ï¼Œå­¦ä¹ éµå¾ªæŒ‡ä»¤\n\n3. äººç±»åé¦ˆå¼ºåŒ–å­¦ä¹  (RLHF/DPO)\n   â””â”€ å¯¹é½äººç±»åå¥½ï¼Œæå‡æœ‰ç”¨æ€§å’Œå®‰å…¨æ€§\n```\n\n### Prompt å·¥ç¨‹\n\n```\n# åŸºæœ¬ç»“æ„\nç³»ç»Ÿæç¤º (System Prompt)\n   â†“\nç”¨æˆ·è¾“å…¥ (User Message)\n   â†“\næ¨¡å‹å›å¤ (Assistant Message)\n```"
            },
            "advanced": {
              "title": "è¿›é˜¶çŸ¥è¯†",
              "content": "### RLHF vs DPO\n\n**RLHFï¼š**\n1. æ”¶é›†äººç±»åå¥½æ•°æ®\n2. è®­ç»ƒå¥–åŠ±æ¨¡å‹\n3. PPOå¼ºåŒ–å­¦ä¹ ä¼˜åŒ–\n\n**DPOï¼ˆæ›´ç®€å•ï¼‰ï¼š**\n```python\n# ç›´æ¥ä¼˜åŒ–åå¥½ï¼Œæ— éœ€å¥–åŠ±æ¨¡å‹\nloss = -log(sigmoid(Î² * (log_Ï€(y_w) - log_Ï€(y_l))))\n```\n\n### é•¿ä¸Šä¸‹æ–‡æŠ€æœ¯\n\n| æŠ€æœ¯ | åŸç† | ä»£è¡¨æ¨¡å‹ |\n|------|------|----------|\n| RoPE | æ—‹è½¬ä½ç½®ç¼–ç  | Llama |\n| ALiBi | çº¿æ€§æ³¨æ„åŠ›åç½® | Bloom |\n| YaRN | RoPEæ’å€¼ | é•¿ä¸Šä¸‹æ–‡Llama |\n| Ring Attention | åˆ†å¸ƒå¼é•¿åºåˆ— | - |\n\n### æ¨ç†åŠ é€Ÿ\n\n```python\n# vLLM - é«˜æ€§èƒ½æ¨ç†\nfrom vllm import LLM, SamplingParams\n\nllm = LLM(model='Qwen/Qwen2.5-7B-Instruct')\nparams = SamplingParams(temperature=0.7, max_tokens=512)\noutputs = llm.generate(['ä½ å¥½'], params)\n```\n\n### é‡åŒ–éƒ¨ç½²\n\n```bash\n# GPTQ 4-bit é‡åŒ–\nmodel = AutoModelForCausalLM.from_pretrained(\n    'model-gptq-4bit',\n    device_map='auto'\n)\n\n# AWQ é‡åŒ–ï¼ˆæ›´å¿«ï¼‰\n# bitsandbytes 8-bit/4-bit\nmodel = AutoModelForCausalLM.from_pretrained(\n    'model',\n    load_in_4bit=True,\n    bnb_4bit_compute_dtype=torch.float16\n)\n```"
            },
            "practice": {
              "title": "å®æˆ˜ä»£ç ",
              "content": "### æœ¬åœ°è¿è¡Œ Ollama\n\n```bash\n# å®‰è£…\ncurl -fsSL https://ollama.ai/install.sh | sh\n\n# è¿è¡Œæ¨¡å‹\nollama run qwen2.5:7b\n\n# Python è°ƒç”¨\nimport requests\n\nresponse = requests.post(\n    'http://localhost:11434/api/generate',\n    json={\n        'model': 'qwen2.5:7b',\n        'prompt': 'ç”¨Pythonå†™ä¸€ä¸ªå¿«é€Ÿæ’åº',\n        'stream': False\n    }\n)\nprint(response.json()['response'])\n```\n\n### OpenAI API è°ƒç”¨\n\n```python\nfrom openai import OpenAI\n\nclient = OpenAI(api_key='sk-xxx')\n\nresponse = client.chat.completions.create(\n    model='gpt-4',\n    messages=[\n        {'role': 'system', 'content': 'ä½ æ˜¯ä¸€ä¸ªhelpfulåŠ©æ‰‹'},\n        {'role': 'user', 'content': 'è§£é‡Šé‡å­è®¡ç®—'}\n    ],\n    temperature=0.7\n)\n\nprint(response.choices[0].message.content)\n```\n\n### LoRA å¾®è°ƒ\n\n```python\nfrom peft import LoraConfig, get_peft_model\n\n# LoRA é…ç½®\nlora_config = LoraConfig(\n    r=64,\n    lora_alpha=128,\n    target_modules=['q_proj', 'k_proj', 'v_proj', 'o_proj'],\n    lora_dropout=0.05,\n    task_type='CAUSAL_LM'\n)\n\n# åº”ç”¨ LoRA\nmodel = get_peft_model(base_model, lora_config)\nmodel.print_trainable_parameters()\n# è¾“å‡º: trainable params: 6M || all params: 7B || 0.09%\n\n# è®­ç»ƒååˆå¹¶\nmerged_model = model.merge_and_unload()\n```\n\n### LangChain RAG\n\n```python\nfrom langchain.embeddings import OpenAIEmbeddings\nfrom langchain.vectorstores import FAISS\nfrom langchain.chains import RetrievalQA\nfrom langchain.llms import OpenAI\n\n# åˆ›å»ºå‘é‡åº“\nembeddings = OpenAIEmbeddings()\nvectorstore = FAISS.from_documents(docs, embeddings)\n\n# åˆ›å»ºé—®ç­”é“¾\nqa = RetrievalQA.from_chain_type(\n    llm=OpenAI(),\n    chain_type='stuff',\n    retriever=vectorstore.as_retriever()\n)\n\nanswer = qa.run('ä»€ä¹ˆæ˜¯Transformerï¼Ÿ')\n```"
            }
          }
        }
      ]
    },
    {
      "id": "multimodal",
      "name": "å¤šæ¨¡æ€AI",
      "icon": "ğŸ¨",
      "chapters": [
        {
          "id": "vision",
          "title": "è§†è§‰ç†è§£",
          "icon": "ğŸ‘ï¸",
          "sections": {
            "terminology": {
              "title": "ä¸“ä¸šæœ¯è¯­",
              "items": [
                {"term": "ViT", "english": "Vision Transformer", "desc": "å°†å›¾åƒåˆ†å—åç”¨Transformerå¤„ç†"},
                {"term": "CLIP", "english": "Contrastive Language-Image Pre-training", "desc": "å›¾æ–‡å¯¹æ¯”å­¦ä¹ ï¼Œå¯¹é½è§†è§‰å’Œè¯­è¨€"},
                {"term": "å¯¹æ¯”å­¦ä¹ ", "english": "Contrastive Learning", "desc": "æ‹‰è¿‘æ­£æ ·æœ¬ï¼Œæ¨è¿œè´Ÿæ ·æœ¬"},
                {"term": "é›¶æ ·æœ¬å­¦ä¹ ", "english": "Zero-shot Learning", "desc": "æ— éœ€è®­ç»ƒæ ·æœ¬å³å¯åˆ†ç±»"},
                {"term": "å›¾åƒåµŒå…¥", "english": "Image Embedding", "desc": "å›¾åƒçš„å‘é‡è¡¨ç¤º"}
              ]
            },
            "basic": {
              "title": "åŸºç¡€æ¦‚å¿µ",
              "content": "### Vision Transformer (ViT)\n\nå°†å›¾åƒåˆ‡åˆ†ä¸º16x16çš„patchï¼Œæ¯ä¸ªpatchä½œä¸ºä¸€ä¸ªtokenï¼š\n\n```\nå›¾åƒ 224x224\n    â†“ åˆ†å—\n14x14 = 196 ä¸ª patch\n    â†“ çº¿æ€§æŠ•å½±\n196 ä¸ª token + 1ä¸ªCLS token\n    â†“ Transformer\nè¾“å‡ºè¡¨ç¤º\n```\n\n### CLIP åŸç†\n\n```\nå›¾åƒç¼–ç å™¨ â”€â”€â†’ å›¾åƒç‰¹å¾\n                   â†˜\n                    å¯¹æ¯”æŸå¤± (æ­£æ ·æœ¬æ¥è¿‘ï¼Œè´Ÿæ ·æœ¬è¿œç¦»)\n                   â†—\næ–‡æœ¬ç¼–ç å™¨ â”€â”€â†’ æ–‡æœ¬ç‰¹å¾\n```\n\né›¶æ ·æœ¬åˆ†ç±»ï¼š\n```python\n# è®¡ç®—å›¾åƒä¸æ‰€æœ‰ç±»åˆ«æ–‡æœ¬çš„ç›¸ä¼¼åº¦\ntext_features = encode(['a photo of a cat', 'a photo of a dog', ...])\nimage_feature = encode(image)\nprobs = softmax(image_feature @ text_features.T)\n```"
            },
            "advanced": {
              "title": "è¿›é˜¶çŸ¥è¯†",
              "content": "### è§†è§‰ç¼–ç å™¨å¯¹æ¯”\n\n| æ¨¡å‹ | å‚æ•°é‡ | ç‰¹ç‚¹ |\n|------|--------|------|\n| ViT-B/16 | 86M | åŸºç¡€ç‰ˆæœ¬ |\n| ViT-L/14 | 304M | CLIPå¸¸ç”¨ |\n| EVA-CLIP | 1B+ | å¼€æºæœ€å¼º |\n| SigLIP | - | Googleæ”¹è¿› |\n| InternViT | 6B | ä¸­æ–‡å¤šæ¨¡æ€ |\n\n### å›¾åƒè¡¨ç¤ºæ–¹æ³•\n\n1. **å…¨å±€ç‰¹å¾**ï¼šCLS token\n2. **å±€éƒ¨ç‰¹å¾**ï¼šæ‰€æœ‰patch token\n3. **å¤šå°ºåº¦ç‰¹å¾**ï¼šä¸åŒå±‚è¾“å‡º\n\n### è§†è§‰ç‰¹å¾æ³¨å…¥LLM\n\n```\næ–¹æ¡ˆ1: æŠ•å½±å±‚\nè§†è§‰ç‰¹å¾ â†’ Linear â†’ LLMè¾“å…¥ç©ºé—´\n\næ–¹æ¡ˆ2: Q-Former (BLIP-2)\nè§†è§‰ç‰¹å¾ â†’ å¯å­¦ä¹ Query â†’ äº¤å‰æ³¨æ„åŠ› â†’ å‹ç¼©è¡¨ç¤º\n\næ–¹æ¡ˆ3: Resampler (Flamingo)\nè§†è§‰ç‰¹å¾ â†’ Perceiver Resampler â†’ å›ºå®šæ•°é‡token\n```"
            },
            "practice": {
              "title": "å®æˆ˜ä»£ç ",
              "content": "### CLIP å›¾åƒåˆ†ç±»\n\n```python\nimport torch\nfrom transformers import CLIPProcessor, CLIPModel\nfrom PIL import Image\n\n# åŠ è½½æ¨¡å‹\nmodel = CLIPModel.from_pretrained('openai/clip-vit-base-patch32')\nprocessor = CLIPProcessor.from_pretrained('openai/clip-vit-base-patch32')\n\n# å‡†å¤‡è¾“å…¥\nimage = Image.open('cat.jpg')\ntexts = ['a photo of a cat', 'a photo of a dog', 'a photo of a bird']\n\ninputs = processor(\n    text=texts,\n    images=image,\n    return_tensors='pt',\n    padding=True\n)\n\n# æ¨ç†\nwith torch.no_grad():\n    outputs = model(**inputs)\n    logits = outputs.logits_per_image\n    probs = logits.softmax(dim=1)\n\nfor text, prob in zip(texts, probs[0]):\n    print(f'{text}: {prob:.2%}')\n```\n\n### å›¾åƒåµŒå…¥æå–\n\n```python\nfrom transformers import AutoModel, AutoProcessor\n\n# ä½¿ç”¨ SigLIP\nmodel = AutoModel.from_pretrained('google/siglip-base-patch16-224')\nprocessor = AutoProcessor.from_pretrained('google/siglip-base-patch16-224')\n\n# æå–å›¾åƒç‰¹å¾\nimage = Image.open('image.jpg')\ninputs = processor(images=image, return_tensors='pt')\n\nwith torch.no_grad():\n    image_features = model.get_image_features(**inputs)\n    image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n\nprint(image_features.shape)  # (1, 768)\n```"
            }
          }
        },
        {
          "id": "vlm",
          "title": "è§†è§‰è¯­è¨€æ¨¡å‹",
          "icon": "ğŸ”—",
          "sections": {
            "terminology": {
              "title": "ä¸“ä¸šæœ¯è¯­",
              "items": [
                {"term": "VLM", "english": "Vision-Language Model", "desc": "èåˆè§†è§‰å’Œè¯­è¨€çš„å¤šæ¨¡æ€æ¨¡å‹"},
                {"term": "LLaVA", "english": "Large Language and Vision Assistant", "desc": "ç®€å•é«˜æ•ˆçš„è§†è§‰è¯­è¨€æ¨¡å‹"},
                {"term": "è§†è§‰æŒ‡ä»¤å¾®è°ƒ", "english": "Visual Instruction Tuning", "desc": "ç”¨å›¾åƒ+æŒ‡ä»¤æ•°æ®å¾®è°ƒ"},
                {"term": "å¤šæ¨¡æ€èåˆ", "english": "Multimodal Fusion", "desc": "èåˆä¸åŒæ¨¡æ€ä¿¡æ¯çš„æŠ€æœ¯"},
                {"term": "å›¾åƒæè¿°", "english": "Image Captioning", "desc": "ä¸ºå›¾åƒç”Ÿæˆæ–‡å­—æè¿°"}
              ]
            },
            "basic": {
              "title": "åŸºç¡€æ¦‚å¿µ",
              "content": "### VLM æ¶æ„æ¼”è¿›\n\n```\nç¬¬ä¸€ä»£: åˆ†ç¦»å¼\nè§†è§‰ç¼–ç å™¨ + è¯­è¨€æ¨¡å‹ + ç®€å•è¿æ¥\nä»£è¡¨: LLaVA-1.0\n\nç¬¬äºŒä»£: æ·±åº¦èåˆ\nè§†è§‰tokenä¸æ–‡æœ¬tokenäº¤å‰æ³¨æ„åŠ›\nä»£è¡¨: Flamingo, BLIP-2\n\nç¬¬ä¸‰ä»£: åŸç”Ÿå¤šæ¨¡æ€\nä»å¤´é¢„è®­ç»ƒçš„ç»Ÿä¸€æ¨¡å‹\nä»£è¡¨: GPT-4V, Gemini\n```\n\n### LLaVA æ¶æ„\n\n```\nå›¾åƒ â†’ CLIP ViT â†’ è§†è§‰ç‰¹å¾\n                      â†“ æŠ•å½±å±‚ (MLP)\n                   è§†è§‰token\n                      â†“\næ–‡æœ¬ â†’ Tokenizer â†’ æ–‡æœ¬token â†’ æ‹¼æ¥ â†’ LLM â†’ è¾“å‡º\n```\n\n### ä¸»æµ VLM å¯¹æ¯”\n\n| æ¨¡å‹ | è§†è§‰ç¼–ç å™¨ | LLM | å¼€æº |\n|------|------------|-----|------|\n| GPT-4V | æœªçŸ¥ | GPT-4 | âŒ |\n| LLaVA-1.6 | CLIP | Llama | âœ… |\n| Qwen-VL | è‡ªç ” | Qwen | âœ… |\n| InternVL | InternViT | InternLM | âœ… |"
            },
            "advanced": {
              "title": "è¿›é˜¶çŸ¥è¯†",
              "content": "### é«˜åˆ†è¾¨ç‡å¤„ç†\n\n**é—®é¢˜**ï¼šViT é€šå¸¸åªæ”¯æŒ 224x224 æˆ– 336x336\n\n**è§£å†³æ–¹æ¡ˆ**ï¼š\n\n1. **åŠ¨æ€åˆ†è¾¨ç‡ (LLaVA-1.6)**\n```\nåŸå›¾ â†’ åˆ†å‰²æˆå¤šä¸ª336x336å—\n     â†’ æ¯å—ç‹¬ç«‹ç¼–ç \n     â†’ æ‹¼æ¥æ‰€æœ‰ç‰¹å¾\n```\n\n2. **åˆ†å±‚é‡‡æ ·**\n```\nå…¨å±€ç‰¹å¾ (ç¼©æ”¾åˆ°å›ºå®šå°ºå¯¸)\n   +\nå±€éƒ¨ç‰¹å¾ (é«˜åˆ†è¾¨ç‡è£å‰ª)\n```\n\n### è§†é¢‘ç†è§£\n\n```python\n# å‡åŒ€é‡‡æ ·å¸§\nframes = sample_frames(video, num_frames=8)\n\n# æ¯å¸§ç¼–ç \nframe_features = [encode(f) for f in frames]\n\n# æ—¶åºå»ºæ¨¡\næ–¹æ¡ˆ1: æ‹¼æ¥æ‰€æœ‰å¸§ç‰¹å¾\næ–¹æ¡ˆ2: æ—¶åºTransformer\næ–¹æ¡ˆ3: 3Då·ç§¯\n```\n\n### è®­ç»ƒæ•°æ®ç±»å‹\n\n1. **å›¾åƒæè¿°**ï¼šç®€å•æè¿°å›¾åƒå†…å®¹\n2. **VQA**ï¼šå›¾åƒé—®ç­”å¯¹\n3. **OCR**ï¼šæ–‡å­—è¯†åˆ«\n4. **å›¾è¡¨ç†è§£**ï¼šChart/è¡¨æ ¼\n5. **å¤šè½®å¯¹è¯**ï¼šå¤æ‚æ¨ç†"
            },
            "practice": {
              "title": "å®æˆ˜ä»£ç ",
              "content": "### ä½¿ç”¨ LLaVA\n\n```python\nfrom transformers import LlavaNextProcessor, LlavaNextForConditionalGeneration\nfrom PIL import Image\nimport torch\n\n# åŠ è½½æ¨¡å‹\nmodel = LlavaNextForConditionalGeneration.from_pretrained(\n    'llava-hf/llava-v1.6-mistral-7b-hf',\n    torch_dtype=torch.float16,\n    device_map='auto'\n)\nprocessor = LlavaNextProcessor.from_pretrained('llava-hf/llava-v1.6-mistral-7b-hf')\n\n# å‡†å¤‡è¾“å…¥\nimage = Image.open('image.jpg')\nconversation = [\n    {\n        'role': 'user',\n        'content': [\n            {'type': 'image'},\n            {'type': 'text', 'text': 'æè¿°è¿™å¼ å›¾ç‰‡'}\n        ]\n    }\n]\n\nprompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\ninputs = processor(prompt, image, return_tensors='pt').to(model.device)\n\n# ç”Ÿæˆ\noutput = model.generate(**inputs, max_new_tokens=512)\nresponse = processor.decode(output[0], skip_special_tokens=True)\nprint(response)\n```\n\n### ä½¿ç”¨ Qwen-VL\n\n```python\nfrom transformers import Qwen2VLForConditionalGeneration, Qwen2VLProcessor\n\nmodel = Qwen2VLForConditionalGeneration.from_pretrained(\n    'Qwen/Qwen2-VL-7B-Instruct',\n    torch_dtype=torch.bfloat16,\n    device_map='auto'\n)\nprocessor = Qwen2VLProcessor.from_pretrained('Qwen/Qwen2-VL-7B-Instruct')\n\nmessages = [\n    {\n        'role': 'user',\n        'content': [\n            {'type': 'image', 'image': 'image.jpg'},\n            {'type': 'text', 'text': 'è¿™æ˜¯ä»€ä¹ˆï¼Ÿ'}\n        ]\n    }\n]\n\ntext = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\ninputs = processor(text=text, images=[Image.open('image.jpg')], return_tensors='pt')\n\noutputs = model.generate(**inputs.to(model.device), max_new_tokens=256)\nprint(processor.decode(outputs[0], skip_special_tokens=True))\n```"
            }
          }
        },
        {
          "id": "image-gen",
          "title": "å›¾åƒç”Ÿæˆ",
          "icon": "ğŸ¨",
          "sections": {
            "terminology": {
              "title": "ä¸“ä¸šæœ¯è¯­",
              "items": [
                {"term": "æ‰©æ•£æ¨¡å‹", "english": "Diffusion Model", "desc": "é€šè¿‡å»å™ªè¿‡ç¨‹ç”Ÿæˆå›¾åƒ"},
                {"term": "Stable Diffusion", "english": "Stable Diffusion", "desc": "åœ¨æ½œç©ºé—´è¿›è¡Œæ‰©æ•£çš„å¼€æºæ¨¡å‹"},
                {"term": "VAE", "english": "Variational Autoencoder", "desc": "ç¼–ç è§£ç å™¨ï¼Œå‹ç¼©å›¾åƒåˆ°æ½œç©ºé—´"},
                {"term": "æ½œç©ºé—´", "english": "Latent Space", "desc": "ä½ç»´å‹ç¼©è¡¨ç¤ºç©ºé—´"},
                {"term": "CFG", "english": "Classifier-Free Guidance", "desc": "æ§åˆ¶ç”Ÿæˆä¸æç¤ºè¯çš„ä¸€è‡´æ€§"},
                {"term": "LoRA", "english": "Low-Rank Adaptation", "desc": "è½»é‡çº§å¾®è°ƒæ–¹æ³•"},
                {"term": "ControlNet", "english": "ControlNet", "desc": "æ·»åŠ é¢å¤–æ§åˆ¶æ¡ä»¶çš„æ¨¡å‹"}
              ]
            },
            "basic": {
              "title": "åŸºç¡€æ¦‚å¿µ",
              "content": "### æ‰©æ•£æ¨¡å‹åŸç†\n\n```\nå‰å‘è¿‡ç¨‹ï¼ˆåŠ å™ªï¼‰:\næ¸…æ™°å›¾åƒ â†’ é€æ­¥åŠ å™ª â†’ çº¯å™ªå£°\n   x_0   â†’    ...    â†’  x_T\n\nåå‘è¿‡ç¨‹ï¼ˆå»å™ªï¼‰:\nçº¯å™ªå£°  â†’ é€æ­¥å»å™ª â†’ æ¸…æ™°å›¾åƒ\n  x_T   â†    ...    â†   x_0\n```\n\n### Stable Diffusion æ¶æ„\n\n```\næ–‡æœ¬ â†’ CLIPæ–‡æœ¬ç¼–ç å™¨ â†’ æ–‡æœ¬åµŒå…¥\n                           â†“\nå™ªå£° â†’ U-Net (å»å™ª) â† äº¤å‰æ³¨æ„åŠ› â†’ æ½œç©ºé—´å›¾åƒ\n                           â†“\n              VAEè§£ç å™¨ â†’ æœ€ç»ˆå›¾åƒ\n```\n\n### å…³é”®å‚æ•°\n\n| å‚æ•° | ä½œç”¨ | æ¨èå€¼ |\n|------|------|--------|\n| Steps | å»å™ªæ­¥æ•° | 20-50 |\n| CFG Scale | æç¤ºè¯æƒé‡ | 7-12 |\n| Sampler | é‡‡æ ·å™¨ | DPM++ 2M |\n| Seed | éšæœºç§å­ | å›ºå®šå¯å¤ç° |"
            },
            "advanced": {
              "title": "è¿›é˜¶çŸ¥è¯†",
              "content": "### SDXL vs SD 1.5\n\n| ç‰¹æ€§ | SD 1.5 | SDXL |\n|------|--------|------|\n| åˆ†è¾¨ç‡ | 512x512 | 1024x1024 |\n| å‚æ•°é‡ | 860M | 2.6B |\n| æ–‡æœ¬ç¼–ç  | CLIP | CLIP + OpenCLIP |\n| è´¨é‡ | è‰¯å¥½ | ä¼˜ç§€ |\n\n### ControlNet ç±»å‹\n\n```\nCanny: è¾¹ç¼˜æ§åˆ¶\nDepth: æ·±åº¦å›¾æ§åˆ¶\nPose: äººä½“å§¿æ€æ§åˆ¶\nSeg: è¯­ä¹‰åˆ†å‰²æ§åˆ¶\nLineart: çº¿ç¨¿æ§åˆ¶\n```\n\n### å›¾åƒç¼–è¾‘æŠ€æœ¯\n\n1. **Inpainting**ï¼šå±€éƒ¨é‡ç»˜\n2. **Outpainting**ï¼šå›¾åƒæ‰©å±•\n3. **Image-to-Image**ï¼šé£æ ¼è½¬æ¢\n4. **Instruct-Pix2Pix**ï¼šæŒ‡ä»¤ç¼–è¾‘\n\n### æç¤ºè¯å·¥ç¨‹\n\n```\næ­£é¢æç¤ºè¯ç»“æ„ï¼š\n[è´¨é‡è¯] + [ä¸»ä½“æè¿°] + [é£æ ¼] + [ç»†èŠ‚]\n\nä¾‹å¦‚ï¼š\nmasterpiece, best quality,\n1girl, long hair, blue eyes, smile,\nanime style, detailed background,\nsoft lighting, 8k\n\nè´Ÿé¢æç¤ºè¯ï¼š\nlowres, bad anatomy, worst quality,\nblurry, watermark, text\n```"
            },
            "practice": {
              "title": "å®æˆ˜ä»£ç ",
              "content": "### Diffusers æ–‡ç”Ÿå›¾\n\n```python\nimport torch\nfrom diffusers import StableDiffusionXLPipeline\n\n# åŠ è½½SDXL\npipe = StableDiffusionXLPipeline.from_pretrained(\n    'stabilityai/stable-diffusion-xl-base-1.0',\n    torch_dtype=torch.float16,\n    variant='fp16'\n).to('cuda')\n\n# ç”Ÿæˆ\nimage = pipe(\n    prompt='a cat wearing sunglasses, photorealistic, 8k',\n    negative_prompt='blurry, low quality',\n    num_inference_steps=30,\n    guidance_scale=7.5,\n    width=1024,\n    height=1024\n).images[0]\n\nimage.save('cat.png')\n```\n\n### åŠ è½½ LoRA\n\n```python\nfrom diffusers import StableDiffusionXLPipeline\n\npipe = StableDiffusionXLPipeline.from_pretrained(...)\n\n# åŠ è½½LoRAæƒé‡\npipe.load_lora_weights('path/to/lora.safetensors')\n\n# è°ƒæ•´LoRAå¼ºåº¦\npipe.fuse_lora(lora_scale=0.8)\n\n# ç”Ÿæˆ\nimage = pipe('è§¦å‘è¯ + æè¿°').images[0]\n```\n\n### ControlNet æ§åˆ¶ç”Ÿæˆ\n\n```python\nfrom diffusers import StableDiffusionXLControlNetPipeline, ControlNetModel\nimport cv2\nimport numpy as np\n\n# Cannyè¾¹ç¼˜æ£€æµ‹\nimage = cv2.imread('input.jpg')\nedges = cv2.Canny(image, 100, 200)\n\n# åŠ è½½ControlNet\ncontrolnet = ControlNetModel.from_pretrained(\n    'diffusers/controlnet-canny-sdxl-1.0',\n    torch_dtype=torch.float16\n)\n\npipe = StableDiffusionXLControlNetPipeline.from_pretrained(\n    'stabilityai/stable-diffusion-xl-base-1.0',\n    controlnet=controlnet,\n    torch_dtype=torch.float16\n).to('cuda')\n\n# ç”Ÿæˆ\nimage = pipe(\n    prompt='anime girl',\n    image=edges,\n    controlnet_conditioning_scale=0.8\n).images[0]\n```\n\n### ComfyUI API è°ƒç”¨\n\n```python\nimport requests\nimport json\n\n# åŠ è½½å·¥ä½œæµ\nwith open('workflow.json') as f:\n    workflow = json.load(f)\n\n# ä¿®æ”¹å‚æ•°\nworkflow['6']['inputs']['text'] = 'your prompt'\n\n# å‘é€è¯·æ±‚\nresponse = requests.post(\n    'http://127.0.0.1:8188/prompt',\n    json={'prompt': workflow}\n)\n```"
            }
          }
        },
        {
          "id": "audio",
          "title": "è¯­éŸ³ä¸éŸ³é¢‘",
          "icon": "ğŸµ",
          "sections": {
            "terminology": {
              "title": "ä¸“ä¸šæœ¯è¯­",
              "items": [
                {"term": "ASR", "english": "Automatic Speech Recognition", "desc": "è¯­éŸ³è½¬æ–‡å­—"},
                {"term": "TTS", "english": "Text-to-Speech", "desc": "æ–‡å­—è½¬è¯­éŸ³"},
                {"term": "Whisper", "english": "Whisper", "desc": "OpenAIçš„å¤šè¯­è¨€è¯­éŸ³è¯†åˆ«æ¨¡å‹"},
                {"term": "è¯­éŸ³å…‹éš†", "english": "Voice Cloning", "desc": "å¤åˆ¶ç‰¹å®šäººçš„å£°éŸ³"},
                {"term": "Melé¢‘è°±", "english": "Mel Spectrogram", "desc": "éŸ³é¢‘çš„æ—¶é¢‘è¡¨ç¤º"}
              ]
            },
            "basic": {
              "title": "åŸºç¡€æ¦‚å¿µ",
              "content": "### è¯­éŸ³AIä»»åŠ¡\n\n```\nè¯­éŸ³è¯†åˆ« (ASR): è¯­éŸ³ â†’ æ–‡å­—\nè¯­éŸ³åˆæˆ (TTS): æ–‡å­— â†’ è¯­éŸ³\nè¯­éŸ³ç¿»è¯‘: è¯­éŸ³Aè¯­è¨€ â†’ æ–‡å­—Bè¯­è¨€\nè¯´è¯äººè¯†åˆ«: è¯†åˆ«è¯´è¯è€…èº«ä»½\nè¯­éŸ³å…‹éš†: å¤åˆ¶å£°éŸ³ç‰¹å¾\n```\n\n### Whisper æ¶æ„\n\n```\néŸ³é¢‘ â†’ Melé¢‘è°± â†’ ç¼–ç å™¨ (Transformer)\n                      â†“\n              è§£ç å™¨ (Transformer) â†’ æ–‡æœ¬\n                      â†‘\n               ä»»åŠ¡token (è½¬å½•/ç¿»è¯‘/æ—¶é—´æˆ³)\n```\n\n### TTS å‘å±•\n\n```\nä¼ ç»Ÿæ–¹æ³•: æ‹¼æ¥åˆæˆã€å‚æ•°åˆæˆ\n    â†“\nç¥ç»ç½‘ç»œ: Tacotron â†’ WaveNet\n    â†“\nç«¯åˆ°ç«¯: VITS, Bark, ChatTTS\n    â†“\nå¤§æ¨¡å‹æ—¶ä»£: GPT-SoVITS, Fish-Speech\n```"
            },
            "advanced": {
              "title": "è¿›é˜¶çŸ¥è¯†",
              "content": "### Whisper æ¨¡å‹è§„æ ¼\n\n| æ¨¡å‹ | å‚æ•°é‡ | è‹±è¯­WER | é€Ÿåº¦ |\n|------|--------|---------|------|\n| tiny | 39M | ~10% | 32x |\n| base | 74M | ~7% | 16x |\n| small | 244M | ~5% | 6x |\n| medium | 769M | ~4% | 2x |\n| large-v3 | 1.5B | ~3% | 1x |\n\n### å®æ—¶è¯­éŸ³æŠ€æœ¯\n\n```\næµå¼ASR:\néŸ³é¢‘æµ â†’ åˆ†å— â†’ æ¨¡å‹æ¨ç† â†’ æµå¼è¾“å‡º\n                  â†“\n            ç¼“å†²åŒºç®¡ç†\n            VADæ£€æµ‹\n\nå®æ—¶TTS:\næ–‡æœ¬ â†’ åˆ†å¥ â†’ å¹¶è¡Œåˆæˆ â†’ éŸ³é¢‘æµ\n```\n\n### è¯­éŸ³å…‹éš†æ–¹æ³•\n\n1. **Zero-shot**: å‡ ç§’å‚è€ƒéŸ³é¢‘å³å¯\n2. **Few-shot**: å‡ åˆ†é’Ÿæ•°æ®å¾®è°ƒ\n3. **Fine-tune**: å®Œæ•´è®­ç»ƒ"
            },
            "practice": {
              "title": "å®æˆ˜ä»£ç ",
              "content": "### Whisper è¯­éŸ³è¯†åˆ«\n\n```python\nimport whisper\n\n# åŠ è½½æ¨¡å‹\nmodel = whisper.load_model('large-v3')\n\n# è½¬å½•\nresult = model.transcribe(\n    'audio.mp3',\n    language='zh',\n    task='transcribe'\n)\n\nprint(result['text'])\n\n# å¸¦æ—¶é—´æˆ³\nfor segment in result['segments']:\n    print(f\"[{segment['start']:.2f} - {segment['end']:.2f}] {segment['text']}\")\n```\n\n### Faster-Whisper (æ›´å¿«)\n\n```python\nfrom faster_whisper import WhisperModel\n\nmodel = WhisperModel('large-v3', device='cuda', compute_type='float16')\n\nsegments, info = model.transcribe('audio.mp3', language='zh')\n\nfor segment in segments:\n    print(f\"[{segment.start:.2f}s -> {segment.end:.2f}s] {segment.text}\")\n```\n\n### Edge-TTS (å…è´¹TTS)\n\n```python\nimport edge_tts\nimport asyncio\n\nasync def text_to_speech(text, output_file):\n    communicate = edge_tts.Communicate(\n        text,\n        voice='zh-CN-XiaoxiaoNeural'  # ä¸­æ–‡å¥³å£°\n    )\n    await communicate.save(output_file)\n\nasyncio.run(text_to_speech('ä½ å¥½ä¸–ç•Œ', 'output.mp3'))\n```\n\n### GPT-SoVITS è¯­éŸ³å…‹éš†\n\n```python\n# å‚è€ƒéŸ³é¢‘ + å‚è€ƒæ–‡æœ¬ â†’ å…‹éš†ç›®æ ‡å£°éŸ³\n\n# 1. å‡†å¤‡å‚è€ƒéŸ³é¢‘ (3-10ç§’æ¸…æ™°è¯­éŸ³)\n# 2. å†™å‡ºå‚è€ƒæ–‡æœ¬ (å¯¹åº”éŸ³é¢‘å†…å®¹)\n# 3. è°ƒç”¨API\n\nimport requests\n\nresponse = requests.post(\n    'http://localhost:9880/tts',\n    json={\n        'text': 'è¦åˆæˆçš„æ–‡æœ¬',\n        'ref_audio': 'reference.wav',\n        'ref_text': 'å‚è€ƒéŸ³é¢‘çš„æ–‡æœ¬å†…å®¹'\n    }\n)\n\nwith open('output.wav', 'wb') as f:\n    f.write(response.content)\n```"
            }
          }
        }
      ]
    },
    {
      "id": "deployment",
      "name": "éƒ¨ç½²ä¸ä¼˜åŒ–",
      "icon": "ğŸš€",
      "chapters": [
        {
          "id": "optimization",
          "title": "æ¨¡å‹ä¼˜åŒ–",
          "icon": "âš¡",
          "sections": {
            "terminology": {
              "title": "ä¸“ä¸šæœ¯è¯­",
              "items": [
                {"term": "é‡åŒ–", "english": "Quantization", "desc": "é™ä½æ•°å€¼ç²¾åº¦å‡å°æ¨¡å‹ä½“ç§¯"},
                {"term": "å‰ªæ", "english": "Pruning", "desc": "ç§»é™¤ä¸é‡è¦çš„æƒé‡"},
                {"term": "çŸ¥è¯†è’¸é¦", "english": "Knowledge Distillation", "desc": "å¤§æ¨¡å‹æ•™å°æ¨¡å‹"},
                {"term": "æ··åˆç²¾åº¦", "english": "Mixed Precision", "desc": "åŒæ—¶ä½¿ç”¨FP16å’ŒFP32"},
                {"term": "ç®—å­èåˆ", "english": "Operator Fusion", "desc": "åˆå¹¶å¤šä¸ªæ“ä½œå‡å°‘å¼€é”€"}
              ]
            },
            "basic": {
              "title": "åŸºç¡€æ¦‚å¿µ",
              "content": "### é‡åŒ–çº§åˆ«\n\n| ç±»å‹ | ç²¾åº¦ | ä½“ç§¯ | è´¨é‡æŸå¤± |\n|------|------|------|----------|\n| FP32 | 32ä½ | 100% | æ—  |\n| FP16 | 16ä½ | 50% | æå° |\n| INT8 | 8ä½ | 25% | å° |\n| INT4 | 4ä½ | 12.5% | ä¸­ç­‰ |\n\n### é‡åŒ–æ–¹æ³•\n\n```\nè®­ç»ƒåé‡åŒ– (PTQ):\nè®­ç»ƒå¥½çš„æ¨¡å‹ â†’ æ ¡å‡†æ•°æ®é›† â†’ é‡åŒ–æ¨¡å‹\nä¼˜ç‚¹: ç®€å•å¿«é€Ÿ\nç¼ºç‚¹: å¯èƒ½æœ‰ç²¾åº¦æŸå¤±\n\né‡åŒ–æ„ŸçŸ¥è®­ç»ƒ (QAT):\nè®­ç»ƒæ—¶æ¨¡æ‹Ÿé‡åŒ– â†’ é‡åŒ–æ¨¡å‹\nä¼˜ç‚¹: ç²¾åº¦æ›´é«˜\nç¼ºç‚¹: éœ€è¦é‡æ–°è®­ç»ƒ\n```\n\n### å¸¸ç”¨é‡åŒ–æ ¼å¼\n\n- **GPTQ**: GPUå‹å¥½ï¼Œ4bitå¸¸ç”¨\n- **AWQ**: æ¿€æ´»æ„ŸçŸ¥ï¼Œè´¨é‡å¥½\n- **GGUF**: llama.cppæ ¼å¼ï¼ŒCPUå‹å¥½\n- **bitsandbytes**: HFé›†æˆï¼Œç®€å•æ˜“ç”¨"
            },
            "advanced": {
              "title": "è¿›é˜¶çŸ¥è¯†",
              "content": "### æ¨ç†ä¼˜åŒ–æŠ€æœ¯æ ˆ\n\n```\næ¨¡å‹å±‚é¢:\nâ”œâ”€â”€ é‡åŒ– (INT8/INT4)\nâ”œâ”€â”€ å‰ªæ (ç»“æ„åŒ–/éç»“æ„åŒ–)\nâ””â”€â”€ è’¸é¦ (æ•™å¸ˆ-å­¦ç”Ÿ)\n\nè®¡ç®—å±‚é¢:\nâ”œâ”€â”€ Flash Attention\nâ”œâ”€â”€ KV Cache\nâ”œâ”€â”€ ç®—å­èåˆ\nâ””â”€â”€ æ‰¹å¤„ç†ä¼˜åŒ–\n\nç³»ç»Ÿå±‚é¢:\nâ”œâ”€â”€ å¼ é‡å¹¶è¡Œ\nâ”œâ”€â”€ æµæ°´çº¿å¹¶è¡Œ\nâ””â”€â”€ åˆ†é¡µæ³¨æ„åŠ› (PagedAttention)\n```\n\n### KV Cache ä¼˜åŒ–\n\n```\né—®é¢˜: æ¯æ¬¡ç”Ÿæˆæ–°tokenéƒ½è¦è®¡ç®—å†å²attention\n\nè§£å†³: ç¼“å­˜å†å²Kã€V\nå†…å­˜å ç”¨ = 2 Ã— num_layers Ã— seq_len Ã— hidden_dim Ã— 2bytes\n\n7Bæ¨¡å‹ï¼Œ2048 seq_len:\n= 2 Ã— 32 Ã— 2048 Ã— 4096 Ã— 2 â‰ˆ 1GB\n\nPagedAttention: åŠ¨æ€åˆ†é…ï¼Œå‡å°‘æµªè´¹\n```\n\n### æœåŠ¡æ¡†æ¶å¯¹æ¯”\n\n| æ¡†æ¶ | ç‰¹ç‚¹ | é€‚ç”¨åœºæ™¯ |\n|------|------|----------|\n| vLLM | PagedAttention | é«˜åå |\n| TGI | HFå®˜æ–¹ | é€šç”¨ |\n| Ollama | ç®€å•æ˜“ç”¨ | æœ¬åœ° |\n| TensorRT-LLM | NVIDIAä¼˜åŒ– | æ€§èƒ½æè‡´ |"
            },
            "practice": {
              "title": "å®æˆ˜ä»£ç ",
              "content": "### bitsandbytes 4-bit é‡åŒ–\n\n```python\nfrom transformers import AutoModelForCausalLM, BitsAndBytesConfig\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type='nf4',\n    bnb_4bit_compute_dtype=torch.bfloat16,\n    bnb_4bit_use_double_quant=True\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    'Qwen/Qwen2.5-7B-Instruct',\n    quantization_config=bnb_config,\n    device_map='auto'\n)\n```\n\n### GPTQ é‡åŒ–\n\n```python\nfrom transformers import AutoModelForCausalLM, GPTQConfig\n\ngptq_config = GPTQConfig(\n    bits=4,\n    dataset='c4',\n    tokenizer=tokenizer\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    'model-name',\n    quantization_config=gptq_config,\n    device_map='auto'\n)\n\n# ä¿å­˜é‡åŒ–æ¨¡å‹\nmodel.save_pretrained('model-gptq-4bit')\n```\n\n### vLLM é«˜æ€§èƒ½æ¨ç†\n\n```python\nfrom vllm import LLM, SamplingParams\n\n# åˆå§‹åŒ–\nllm = LLM(\n    model='Qwen/Qwen2.5-7B-Instruct',\n    tensor_parallel_size=1,  # GPUæ•°é‡\n    gpu_memory_utilization=0.9\n)\n\n# æ‰¹é‡æ¨ç†\nparams = SamplingParams(\n    temperature=0.7,\n    top_p=0.9,\n    max_tokens=512\n)\n\nprompts = ['é—®é¢˜1', 'é—®é¢˜2', 'é—®é¢˜3']\noutputs = llm.generate(prompts, params)\n\nfor output in outputs:\n    print(output.outputs[0].text)\n```\n\n### llama.cpp é‡åŒ–\n\n```bash\n# è½¬æ¢ä¸ºGGUF\npython convert_hf_to_gguf.py model_dir --outtype f16\n\n# é‡åŒ–\n./quantize model.gguf model-q4_k_m.gguf q4_k_m\n\n# è¿è¡Œ\n./llama-cli -m model-q4_k_m.gguf -p \"Hello\" -n 128\n```"
            }
          }
        },
        {
          "id": "serving",
          "title": "æ¨¡å‹éƒ¨ç½²",
          "icon": "ğŸŒ",
          "sections": {
            "terminology": {
              "title": "ä¸“ä¸šæœ¯è¯­",
              "items": [
                {"term": "æ¨ç†æœåŠ¡", "english": "Inference Serving", "desc": "æä¾›æ¨¡å‹æ¨ç†çš„æœåŠ¡"},
                {"term": "APIç½‘å…³", "english": "API Gateway", "desc": "ç®¡ç†å’Œè·¯ç”±APIè¯·æ±‚"},
                {"term": "è´Ÿè½½å‡è¡¡", "english": "Load Balancing", "desc": "åˆ†å‘è¯·æ±‚åˆ°å¤šä¸ªå®ä¾‹"},
                {"term": "è‡ªåŠ¨æ‰©ç¼©", "english": "Auto Scaling", "desc": "æ ¹æ®è´Ÿè½½è‡ªåŠ¨è°ƒæ•´å®ä¾‹æ•°"},
                {"term": "å®¹å™¨åŒ–", "english": "Containerization", "desc": "ä½¿ç”¨Dockeræ‰“åŒ…åº”ç”¨"}
              ]
            },
            "basic": {
              "title": "åŸºç¡€æ¦‚å¿µ",
              "content": "### éƒ¨ç½²æ¶æ„\n\n```\nå®¢æˆ·ç«¯\n   â†“\nAPIç½‘å…³ (Nginx/Kong)\n   â†“\nè´Ÿè½½å‡è¡¡\n   â†“\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚  æ¨ç†æœåŠ¡é›†ç¾¤           â”‚\nâ”‚  â”œâ”€â”€ vLLMå®ä¾‹1          â”‚\nâ”‚  â”œâ”€â”€ vLLMå®ä¾‹2          â”‚\nâ”‚  â””â”€â”€ vLLMå®ä¾‹3          â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n   â†“\næ¨¡å‹å­˜å‚¨ (S3/OSS)\n```\n\n### éƒ¨ç½²æ–¹å¼å¯¹æ¯”\n\n| æ–¹å¼ | ä¼˜ç‚¹ | ç¼ºç‚¹ |\n|------|------|------|\n| æœ¬åœ° | ç®€å• | ä¸å¯æ‰©å±• |\n| Docker | å¯ç§»æ¤ | éœ€è¦ç¼–æ’ |\n| K8s | å¯æ‰©å±• | å¤æ‚ |\n| Serverless | æŒ‰éœ€ä»˜è´¹ | å†·å¯åŠ¨ |\n| äº‘API | é›¶è¿ç»´ | æˆæœ¬é«˜ |"
            },
            "advanced": {
              "title": "è¿›é˜¶çŸ¥è¯†",
              "content": "### Kubernetes éƒ¨ç½²\n\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: llm-service\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: llm\n  template:\n    spec:\n      containers:\n      - name: vllm\n        image: vllm/vllm-openai:latest\n        resources:\n          limits:\n            nvidia.com/gpu: 1\n        args:\n          - --model=Qwen/Qwen2.5-7B-Instruct\n          - --port=8000\n```\n\n### ç›‘æ§æŒ‡æ ‡\n\n```\næ ¸å¿ƒæŒ‡æ ‡:\nâ”œâ”€â”€ QPS (æ¯ç§’è¯·æ±‚æ•°)\nâ”œâ”€â”€ Latency (å»¶è¿Ÿ)\nâ”‚   â”œâ”€â”€ P50, P90, P99\nâ”‚   â”œâ”€â”€ TTFT (é¦–tokenæ—¶é—´)\nâ”‚   â””â”€â”€ TPS (æ¯ç§’tokenæ•°)\nâ”œâ”€â”€ GPUåˆ©ç”¨ç‡\nâ””â”€â”€ å†…å­˜ä½¿ç”¨\n```\n\n### æˆæœ¬ä¼˜åŒ–\n\n1. **Spotå®ä¾‹**: èŠ‚çœ70%\n2. **æ¨¡å‹é‡åŒ–**: å‡å°‘GPUéœ€æ±‚\n3. **æ‰¹å¤„ç†**: æé«˜åå\n4. **æ™ºèƒ½è·¯ç”±**: ç®€å•è¯·æ±‚ç”¨å°æ¨¡å‹"
            },
            "practice": {
              "title": "å®æˆ˜ä»£ç ",
              "content": "### FastAPI + vLLM\n\n```python\nfrom fastapi import FastAPI\nfrom vllm import LLM, SamplingParams\nfrom pydantic import BaseModel\n\napp = FastAPI()\nllm = LLM(model='Qwen/Qwen2.5-7B-Instruct')\n\nclass ChatRequest(BaseModel):\n    messages: list\n    temperature: float = 0.7\n    max_tokens: int = 512\n\n@app.post('/v1/chat/completions')\nasync def chat(request: ChatRequest):\n    # æ„å»ºprompt\n    prompt = format_messages(request.messages)\n    \n    params = SamplingParams(\n        temperature=request.temperature,\n        max_tokens=request.max_tokens\n    )\n    \n    outputs = llm.generate([prompt], params)\n    \n    return {\n        'choices': [{\n            'message': {\n                'role': 'assistant',\n                'content': outputs[0].outputs[0].text\n            }\n        }]\n    }\n```\n\n### Docker éƒ¨ç½²\n\n```dockerfile\n# Dockerfile\nFROM nvidia/cuda:12.1-runtime-ubuntu22.04\n\nRUN pip install vllm fastapi uvicorn\n\nCOPY app.py /app/\nWORKDIR /app\n\nCMD [\"uvicorn\", \"app:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n```\n\n```bash\n# æ„å»ºå’Œè¿è¡Œ\ndocker build -t llm-service .\ndocker run --gpus all -p 8000:8000 llm-service\n```\n\n### Nginx è´Ÿè½½å‡è¡¡\n\n```nginx\nupstream llm_backend {\n    least_conn;\n    server llm1:8000 weight=1;\n    server llm2:8000 weight=1;\n    server llm3:8000 weight=1;\n}\n\nserver {\n    listen 80;\n    \n    location /v1/ {\n        proxy_pass http://llm_backend;\n        proxy_http_version 1.1;\n        proxy_set_header Connection '';\n        proxy_read_timeout 300s;\n    }\n}\n```\n\n### äº‘æœåŠ¡å•† SDK\n\n```python\n# é˜¿é‡Œäº‘ç™¾ç‚¼\nfrom dashscope import Generation\n\nresponse = Generation.call(\n    model='qwen-turbo',\n    messages=[{'role': 'user', 'content': 'ä½ å¥½'}]\n)\n\n# ç«å±±å¼•æ“\nfrom volcengine.maas import MaasService\n\nmaas = MaasService('maas-api.ml-platform-cn-beijing.volces.com')\nresp = maas.chat(model_id, messages)\n```"
            }
          }
        }
      ]
    }
  ]
}
